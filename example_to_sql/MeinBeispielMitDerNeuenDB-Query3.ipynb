{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Mein Beispiel wird hier stattfinden. Mit der neuen tpch database und einer einfachen query zuerst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from inspect import cleandoc\n",
    "from mlinspect import PipelineInspector\n",
    "from mlinspect.utils import get_project_root\n",
    "from mlinspect.to_sql.dbms_connectors.postgresql_connector import PostgresqlConnector\n",
    "import os\n",
    "import time\n",
    "\n",
    "##Mal schauen ob sich das problem löst, wenn ich alle sachen importiere\n",
    "from mlinspect.inspections import HistogramForColumns, RowLineage, MaterializeFirstOutputRows\n",
    "from mlinspect.checks import NoBiasIntroducedFor, NoIllegalFeatures\n",
    "from demo.feature_overview.no_missing_embeddings import NoMissingEmbeddings\n",
    "from example_pipelines.healthcare import custom_monkeypatching\n",
    "from mlinspect import OperatorType\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "POSTGRES_USER = \"postgres\"\n",
    "POSTGRES_PW = \"123456\"\n",
    "POSTGRES_DB = \"db1\"\n",
    "POSTGRES_PORT = 25432\n",
    "POSTGRES_HOST = \"localhost\"\n",
    "\n",
    "#POSTGRES_USER = \"luca\"\n",
    "#POSTGRES_PW = \"password\"\n",
    "#POSTGRES_DB = \"healthcare_benchmark\"\n",
    "#POSTGRES_PORT = 5432\n",
    "#POSTGRES_HOST = \"localhost\"\n",
    "\n",
    "dbms_connector_p = PostgresqlConnector(dbname=POSTGRES_DB, user=POSTGRES_USER, password=POSTGRES_PW,\n",
    "                                       port=POSTGRES_PORT, host=POSTGRES_HOST)\n",
    "\n",
    "print(dbms_connector_p.db_settings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import warnings\n",
    "import os\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import modin.pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from example_pipelines.healthcare.healthcare_utils import MyW2VTransformer, MyKerasClassifier,         create_model\n",
    "from mlinspect.utils import get_project_root, store_timestamp\n",
    "import time\n",
    "from mlinspect.to_sql.dbms_connectors.postgresql_connector import PostgresqlConnector\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "\n",
    "alchemyEngine = create_engine(\n",
    "    'postgresql+psycopg2://postgres:123456@127.0.0.1:25432/db1', pool_recycle=3600)\n",
    "\n",
    "alchemyEngine2 = create_engine(\n",
    "    'postgresql+psycopg2://postgres:123456@127.0.0.1:15432/db1', pool_recycle=3600)\n",
    "\n",
    "def udf_disc_price(extended, discount):\n",
    "    return (extended*(1-discount))\n",
    "\n",
    "def udf_charge(extended, discount, tax):\n",
    "    return extended*((1-discount)*(1+tax))\n",
    "    #np.multiply(extended, np.multiply(np.subtract(1, discount), np.add(1, tax)))\n",
    "           \n",
    "def load_table(table, dbms_con, sf=1):\n",
    "    parse_dates=[]\n",
    "    if 'orders' in table:\n",
    "        parse_dates=['o_orderdate']\n",
    "    elif 'lineitem' in table:\n",
    "        parse_dates=['l_shipdate', 'l_commitdate', 'l_receiptdate']\n",
    "\n",
    "    df = pd.read_sql_query('SELECT * FROM '+table,\n",
    "                         dbms_con, parse_dates=parse_dates)\n",
    "    \n",
    "    cols = df.select_dtypes(object).columns\n",
    "    df[cols] = df[cols].apply(lambda x: x.astype(str).str.strip())\n",
    "    return df\n",
    "\n",
    "\n",
    "t0 = time.time()    \n",
    "\n",
    "\n",
    "orders = load_table('pg1_sf1_orders', alchemyEngine2)\n",
    "customer = load_table('pg2_sf1_customer', alchemyEngine)\n",
    "lineitem = load_table('pg2_sf1_lineitem', alchemyEngine)\n",
    "\n",
    "t1 = time.time()    \n",
    "\n",
    "o = orders[[\"o_orderkey\", \"o_custkey\", \"o_orderdate\", \"o_shippriority\"]][orders[\"o_orderdate\"] <\n",
    "                                                                         \"1995-03-15\"][[\"o_orderkey\", \"o_custkey\", \"o_orderdate\", \"o_shippriority\"]]\n",
    "c = customer[[\"c_custkey\", \"c_mktsegment\"]][customer[\"c_mktsegment\"] ==\n",
    "                                            \"BUILDING\"][[\"c_custkey\", \"c_mktsegment\"]]\n",
    "oc = o.merge(c, left_on=\"o_custkey\", right_on=\"c_custkey\")[\n",
    "    [\"o_orderkey\", \"o_orderdate\", \"o_shippriority\"]]\n",
    "l = lineitem[[\"l_orderkey\", \"l_extendedprice\", \"l_discount\", \"l_shipdate\"]\n",
    "             ][lineitem[\"l_shipdate\"] > \"1995-03-15\"][[\"l_orderkey\", \"l_extendedprice\", \"l_discount\"]]\n",
    "loc = l.merge(oc, left_on=\"l_orderkey\", right_on=\"o_orderkey\")\n",
    "loc[\"volume\"] = loc[\"l_extendedprice\"] * (1 - loc[\"l_discount\"])\n",
    "res = loc.groupby([\"l_orderkey\", \"o_orderdate\", \"o_shippriority\"]).agg({'volume': sum}).reset_index()[\n",
    "    [\"l_orderkey\", \"volume\", \"o_orderdate\", \"o_shippriority\"]].sort_values([\"volume\", \"o_orderdate\"], ascending=[False, True]).head(10)\n",
    "\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"\\nTime spend with modified SQL inspections (PSQL): \" + str(t2 - t0))\n",
    "print(\"\\nTime spend for loading: \" + str(t1 - t0))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline_code = cleandoc(\"\"\"\n",
    "    import warnings\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "    from example_pipelines.healthcare.healthcare_utils import MyW2VTransformer, MyKerasClassifier, \\\n",
    "        create_model\n",
    "    from mlinspect.utils import get_project_root, store_timestamp\n",
    "    import time\n",
    "    from mlinspect.to_sql.dbms_connectors.postgresql_connector import PostgresqlConnector\n",
    "    from sqlalchemy import create_engine\n",
    "\n",
    "    #HIER IST DIE ECHTE TESTDB MIT TPCH DATEN\n",
    "    POSTGRES_USER = \"postgres\"\n",
    "    POSTGRES_PW = \"123456\"\n",
    "    POSTGRES_DB = \"db1\"\n",
    "    POSTGRES_PORT = 25432\n",
    "    POSTGRES_HOST = \"localhost\"\n",
    "    \n",
    "    POSTGRES_USER2 = \"postgres\"\n",
    "    POSTGRES_PW2 = \"123456\"\n",
    "    POSTGRES_DB2 = \"db1\"\n",
    "    POSTGRES_PORT2 = 15432\n",
    "    POSTGRES_HOST2 = \"localhost\"\n",
    "    \n",
    "    dbms_connector_p2 = PostgresqlConnector(dbname=POSTGRES_DB2, user=POSTGRES_USER2, password=POSTGRES_PW2, port=POSTGRES_PORT2, host=POSTGRES_HOST2)\n",
    "\n",
    "    dbms_connector_p = PostgresqlConnector(dbname=POSTGRES_DB, user=POSTGRES_USER, password=POSTGRES_PW, port=POSTGRES_PORT, host=POSTGRES_HOST)\n",
    "    \n",
    "    alchemyEngine = create_engine(\n",
    "        'postgresql+psycopg2://postgres:123456@127.0.0.1:25432/db1', pool_recycle=3600)\n",
    "\n",
    "    alchemyEngine2 = create_engine(\n",
    "        'postgresql+psycopg2://postgres:123456@127.0.0.1:15432/db1', pool_recycle=3600)\n",
    "    \n",
    "    mariaEngine = create_engine(\"mysql+mysqldb://mariadb:123456@127.0.0.1:13306/mdb\")\n",
    "    \n",
    "    def udf_disc_price(extended, discount):\n",
    "        return (extended*(1-discount))\n",
    "\n",
    "    def udf_charge(extended, discount, tax):\n",
    "        return extended*((1-discount)*(1+tax))\n",
    "        #np.multiply(extended, np.multiply(np.subtract(1, discount), np.add(1, tax)))\n",
    "               \n",
    "    def load_table(table, dbms_con, sf=1):\n",
    "        parse_dates=[]\n",
    "        if 'orders' in table:\n",
    "            parse_dates=['o_orderdate']\n",
    "        elif 'lineitem' in table:\n",
    "            parse_dates=['l_shipdate', 'l_commitdate', 'l_receiptdate']\n",
    "\n",
    "        #df = pd.read_sql_query('SELECT * FROM pg2_sf'+str(sf)+'_'+table,\n",
    "        #                     dbms_connector_p.connection, parse_dates=parse_dates)\n",
    "        #df = pd.read_sql_query('SELECT * FROM '+table,\n",
    "        #                     dbms_con.connection, parse_dates=parse_dates)\n",
    "        df = pd.read_sql_query('SELECT * FROM '+table,\n",
    "                             dbms_con, parse_dates=parse_dates)\n",
    "        #dbms_connector_p.run(f\"END;\")\n",
    "        #pd.read_sql_query('SELECT * FROM '+table,\n",
    "        #                    dbms_con, parse_dates=parse_dates)\n",
    "        #dbms_con.run(f\"END;\")\n",
    "        \n",
    "        #cols = df.select_dtypes(object).columns\n",
    "        #df[cols] = df[cols].apply(lambda x: x.astype(str).str.strip())\n",
    "        return df\n",
    "   \n",
    "    #XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "    #Query 1\n",
    "    #parse_dates=['l_shipdate', 'l_commitdate', 'l_receiptdate']\n",
    "    #lineitem=pd.read_sql_query('SELECT * FROM pg2_sf1_lineitem', dbms_connector_p.connection, parse_dates=parse_dates)\n",
    "    #\n",
    "    #df=lineitem[[\"l_shipdate\", \"l_returnflag\", \"l_linestatus\", \"l_quantity\",\n",
    "    #                \"l_extendedprice\", \"l_discount\", \"l_tax\"]][(lineitem['l_shipdate'] <= '1998-09-01')]\n",
    "    #df['disc_price'] = udf_disc_price(\n",
    "    #    df['l_extendedprice'], df['l_discount'])\n",
    "    #df['charge'] = udf_charge(df['l_extendedprice'],\n",
    "    #                            df['l_discount'], df['l_tax'])    \n",
    "    #res = df.groupby(['l_returnflag', 'l_linestatus'])\\\n",
    "    #    .agg({'l_quantity': 'sum', 'l_extendedprice': 'sum', 'disc_price': 'sum', 'charge': 'sum',\n",
    "    #            'l_quantity': 'mean', 'l_extendedprice': 'mean', 'l_discount': 'mean', 'l_shipdate': 'count'})\n",
    "    #\n",
    "    #print(\"df jjjjjjjjjjjjjjjjjjjjjjjjjjjj\", df)\n",
    "    #print(\"res jjjjjjjjjjjjjjjjjjjjjjjjjj\", res)\n",
    "    \n",
    "    \\\"\\\"\\\"\n",
    "    #Query 1 with methods. Ist nur eine tabelle´also irrelevant.\n",
    "    lineitem = load_table('lineitem', 1)\n",
    "    \n",
    "    df = lineitem[[\"l_shipdate\", \"l_returnflag\", \"l_linestatus\", \"l_quantity\",\n",
    "                   \"l_extendedprice\", \"l_discount\", \"l_tax\"]][(lineitem['l_shipdate'] <= '1998-09-01')]\n",
    "    df['disc_price'] = udf_disc_price(\n",
    "        df['l_extendedprice'], df['l_discount'])\n",
    "    df['charge'] = udf_charge(df['l_extendedprice'],\n",
    "                              df['l_discount'], df['l_tax'])\n",
    "    res = df.groupby(['l_returnflag', 'l_linestatus'])\\\n",
    "        .agg({'l_quantity': 'sum', 'l_extendedprice': 'sum', 'disc_price': 'sum', 'charge': 'sum',\n",
    "              'l_quantity': 'mean', 'l_extendedprice': 'mean', 'l_discount': 'mean', 'l_shipdate': 'count'})\n",
    "    \n",
    "    print(\"df jjjjjjjjjjjjjjjjjjjjjjjjjjjj\", df)\n",
    "    print(\"res jjjjjjjjjjjjjjjjjjjjjjjjjj\", res)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    #Query 2 Funktioniert nicht weil SQLGLOT query falsch optimized.\n",
    "    nation = load_table('pg2_sf1_nation', dbms_connector_p)\n",
    "    part = load_table('pg2_sf1_part', dbms_connector_p)\n",
    "    supplier = load_table('pg2_sf1_supplier', dbms_connector_p)\n",
    "    region = load_table('pg2_sf1_region', dbms_connector_p)\n",
    "    partsupp = load_table('pg2_sf1_partsupp',dbms_connector_p)\n",
    "    \n",
    "    ps = partsupp[[\"ps_partkey\", \"ps_suppkey\", \"ps_supplycost\"]]\n",
    "    p = part[[\"p_partkey\", \"p_mfgr\", \"p_size\", \"p_type\"]][(part['p_size'] == 15) & (\n",
    "        part['p_type'].str.match(\".*BRASS$\"))][[\"p_partkey\", \"p_mfgr\"]]\n",
    "        \n",
    "    psp = ps.merge(p, left_on=\"ps_partkey\", right_on=\"p_partkey\")\n",
    "    s = supplier[[\"s_suppkey\", \"s_nationkey\", \"s_acctbal\",\n",
    "                  \"s_name\", \"s_address\", \"s_phone\", \"s_comment\"]]\n",
    "    psps = psp.merge(s, left_on=\"ps_suppkey\", right_on=\"s_suppkey\")[\n",
    "        [\"ps_partkey\", \"ps_supplycost\", \"p_mfgr\", \"s_nationkey\",         \"s_acctbal\", \"s_name\", \"s_address\", \"s_phone\", \"s_comment\"]]\n",
    "    nr = nation.merge(region[region[\"r_name\"] == \"EUROPE\"], left_on=\"n_regionkey\", right_on=\"r_regionkey\")[[\"n_nationkey\", \"n_name\"]]\n",
    "    \n",
    "    pspsnr = psps.merge(nr, left_on=\"s_nationkey\", right_on=\"n_nationkey\")[[\"ps_partkey\", \"ps_supplycost\", \"p_mfgr\", \"n_name\", \"s_acctbal\", \"s_name\", \"s_address\", \"s_phone\", \"s_comment\"]]\n",
    "    aggr = pspsnr.groupby(\"ps_partkey\").agg({'ps_supplycost': min}).reset_index()\n",
    "    sj = pspsnr.merge(aggr, left_on=[\"ps_partkey\", \"ps_supplycost\"], right_on=[\"ps_partkey\", \"ps_supplycost\"])\n",
    "    res = sj[[\"s_acctbal\", \"s_name\", \"n_name\", \"ps_partkey\", \"p_mfgr\", \"s_address\", \"s_phone\", \"s_comment\"]].sort_values([\"s_acctbal\", \"n_name\", \"s_name\", \"ps_partkey\"], ascending=[False, True, True, True]).head(100)\n",
    "    print(\"res jjjjjjjjjjjjjjjjjjjjjjjjjj\", res)\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Query 3 FUNKTIONIERT\n",
    "    \n",
    "    orders = load_table('pg1_sf1_orders', alchemyEngine2)\n",
    "    #orders = load_table('mdb1_sf1_orders', mariaEngine)\n",
    "    customer = load_table('pg2_sf1_customer', alchemyEngine)\n",
    "    lineitem = load_table('pg2_sf1_lineitem', alchemyEngine)\n",
    "\n",
    "    o = orders[[\"o_orderkey\", \"o_custkey\", \"o_orderdate\", \"o_shippriority\"]][orders[\"o_orderdate\"] <\n",
    "                                                                             \"1995-03-15\"][[\"o_orderkey\", \"o_custkey\", \"o_orderdate\", \"o_shippriority\"]]\n",
    "    c = customer[[\"c_custkey\", \"c_mktsegment\"]][customer[\"c_mktsegment\"] ==\n",
    "                                                \"BUILDING\"][[\"c_custkey\", \"c_mktsegment\"]]\n",
    "    oc = o.merge(c, left_on=\"o_custkey\", right_on=\"c_custkey\")[\n",
    "        [\"o_orderkey\", \"o_orderdate\", \"o_shippriority\"]]\n",
    "    l = lineitem[[\"l_orderkey\", \"l_extendedprice\", \"l_discount\", \"l_shipdate\"]\n",
    "                 ][lineitem[\"l_shipdate\"] > \"1995-03-15\"][[\"l_orderkey\", \"l_extendedprice\", \"l_discount\"]]\n",
    "    loc = l.merge(oc, left_on=\"l_orderkey\", right_on=\"o_orderkey\")\n",
    "    loc[\"volume\"] = loc[\"l_extendedprice\"] * (1 - loc[\"l_discount\"])\n",
    "    res = loc.groupby([\"l_orderkey\", \"o_orderdate\", \"o_shippriority\"]).agg({'volume': sum}).reset_index()[\n",
    "        [\"l_orderkey\", \"volume\", \"o_orderdate\", \"o_shippriority\"]].sort_values([\"volume\", \"o_orderdate\"], ascending=[False, True]).head(10)\n",
    "    \n",
    "    print(\"res jjjjjjjjjjjjjjjjjjjjjjjjjj\", res)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "\n",
    "    #Query 4 FUNKTIONIERT NICHT WEGEN DROP_DUPLICATES UND SIZE().\n",
    "                \n",
    "    \n",
    "    orders = load_table('pg2_sf1_orders', dbms_connector_p)\n",
    "    lineitem = load_table('pg2_sf1_lineitem', dbms_connector_p)\n",
    "\n",
    "    l = lineitem[[\"l_orderkey\", \"l_commitdate\"]\n",
    "                 ][lineitem[\"l_commitdate\"] < lineitem[\"l_receiptdate\"]][[\"l_orderkey\"]]\n",
    "    o = orders[[\"o_orderkey\", \"o_orderpriority\", \"o_orderdate\"]][(\n",
    "        orders[\"o_orderdate\"] >= \"1993-07-01\") & (orders[\"o_orderdate\"] < \"1993-10-01\")][[\"o_orderkey\", \"o_orderpriority\"]]\n",
    "    #lo = l.merge(o, left_on=\"l_orderkey\", right_on=\"o_orderkey\").drop_duplicates()[\n",
    "    #    [\"o_orderpriority\"]]\n",
    "    lo = l.merge(o, left_on=\"l_orderkey\", right_on=\"o_orderkey\")[\n",
    "        [\"o_orderpriority\"]]\n",
    "    #res = lo.groupby(\"o_orderpriority\").size().reset_index(\n",
    "    #    name='counts').sort_values('o_orderpriority')\n",
    "    res = lo.groupby(\"o_orderpriority\")\n",
    "    \n",
    "    \\\"\\\"\\\"       \n",
    "    #Query 5 FUNKTIONIERT!!!!\n",
    "\n",
    "    \n",
    "    region = load_table('pg2_sf1_region', alchemyEngine)\n",
    "    nation = load_table('pg2_sf1_nation', alchemyEngine)\n",
    "    supplier = load_table('pg1_sf1_supplier', alchemyEngine2)\n",
    "    orders = load_table('pg1_sf1_orders', alchemyEngine2)\n",
    "    lineitem = load_table('pg2_sf1_lineitem', alchemyEngine)\n",
    "    customer = load_table('pg1_sf1_customer', alchemyEngine2)\n",
    "\n",
    "\n",
    "    nr = nation.merge(region[region[\"r_name\"] == \"ASIA\"], left_on=\"n_regionkey\",\n",
    "                      right_on=\"r_regionkey\")[[\"n_nationkey\", \"n_name\"]]\n",
    "    snr = supplier[[\"s_suppkey\", \"s_nationkey\"]].merge(\n",
    "        nr, left_on=\"s_nationkey\", right_on=\"n_nationkey\")[[\"s_suppkey\", \"s_nationkey\", \"n_name\"]]\n",
    "    lsnr = lineitem[[\"l_suppkey\", \"l_orderkey\", \"l_extendedprice\", \"l_discount\"]].merge(\n",
    "        snr, left_on=\"l_suppkey\", right_on=\"s_suppkey\")\n",
    "    o = orders[[\"o_orderkey\", \"o_custkey\", \"o_orderdate\"]][(\n",
    "        orders[\"o_orderdate\"] >= \"1994-01-01\") & (orders[\"o_orderdate\"] < \"1995-01-01\")][[\"o_orderkey\", \"o_custkey\"]]\n",
    "    oc = o.merge(customer[[\"c_custkey\", \"c_nationkey\"]], left_on=\"o_custkey\",\n",
    "                 right_on=\"c_custkey\")[[\"o_orderkey\", \"c_nationkey\"]]\n",
    "    lsnroc = lsnr.merge(oc, left_on=[\"l_orderkey\", \"s_nationkey\"], right_on=[\n",
    "                        \"o_orderkey\", \"c_nationkey\"])[[\"l_extendedprice\", \"l_discount\", \"n_name\"]]\n",
    "    lsnroc[\"volume\"] = lsnroc[\"l_extendedprice\"] * (1 - lsnroc[\"l_discount\"])\n",
    "    res = lsnroc.groupby(\"n_name\").agg(\n",
    "        {'volume': sum}).reset_index().sort_values(\"volume\", ascending=False)\n",
    "        \n",
    "\n",
    "\n",
    "    # Query 10 Funktioniert nicht weil groupby gemacht wird und dann wird gemerged. Im code müsste das wie mehrere Group-by statements aussehen, aber wird natürlich nicht gemacht, weil in pipeline anders.\n",
    "    orders = load_table('pg2_sf1_orders', dbms_connector_p)\n",
    "    lineitem = load_table('pg2_sf1_lineitem', dbms_connector_p)\n",
    "    customer = load_table('pg2_sf1_customer', dbms_connector_p)\n",
    "    nation = load_table('pg2_sf1_nation', dbms_connector_p)\n",
    "\n",
    "    l = lineitem[[\"l_orderkey\", \"l_extendedprice\", \"l_discount\", \"l_returnflag\"]\n",
    "                 ][lineitem[\"l_returnflag\"] == \"R\"][[\"l_orderkey\", \"l_extendedprice\", \"l_discount\"]]\n",
    "    o = orders[[\"o_orderkey\", \"o_custkey\", \"o_orderdate\"]][(\n",
    "        orders[\"o_orderdate\"] >= \"1993-10-01\") & (orders[\"o_orderdate\"] < \"1994-01-01\")][[\"o_orderkey\", \"o_custkey\"]]\n",
    "    lo = l.merge(o, left_on=\"l_orderkey\", right_on=\"o_orderkey\")[\n",
    "        [\"l_extendedprice\", \"l_discount\", \"o_custkey\"]]\n",
    "    lo[\"volume\"] = lo[\"l_extendedprice\"] * (1 - lo[\"l_discount\"])\n",
    "    lo_aggr = lo.groupby(\"o_custkey\").agg({'volume': sum}).reset_index()\n",
    "    c = customer[[\"c_custkey\", \"c_nationkey\", \"c_name\",\n",
    "                  \"c_acctbal\", \"c_phone\", \"c_address\", \"c_comment\"]]\n",
    "    loc = lo_aggr.merge(c, left_on=\"o_custkey\", right_on=\"c_custkey\")\n",
    "    locn = loc.merge(nation[[\"n_nationkey\", \"n_name\"]],\n",
    "                     left_on=\"c_nationkey\", right_on=\"n_nationkey\")\n",
    "    res = locn[[\"o_custkey\", \"c_name\", \"volume\", \"c_acctbal\", \"n_name\", \"c_address\",\n",
    "                \"c_phone\", \"c_comment\"]].sort_values(\"volume\", ascending=False).head(20)\n",
    "\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "print(pipeline_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## The function to retrieve the generated code:\n",
    "def get_sql_query(pipeline_code, to_sql, dbms_connector=None, mode=None, materialize=None):\n",
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mlinspect.visualisation import save_fig_to_path\n",
    "    import os\n",
    "    import sys\n",
    "    \n",
    "    inspector_result = PipelineInspector \\\n",
    "        .on_pipeline_from_string(pipeline_code) \n",
    "    \n",
    "    if to_sql:\n",
    "        inspector_result = inspector_result.execute_in_sql(dbms_connector=dbms_connector, mode=mode, materialize=materialize)\n",
    "    else:\n",
    "        inspector_result = inspector_result.execute()\n",
    "        return None\n",
    "\n",
    "    #extracted_dag = inspector_result.dag\n",
    "    #filename = os.path.join(str(get_project_root()), \"demo\", \"feature_overview\", \"newTest.png\")\n",
    "    #save_fig_to_path(extracted_dag, filename)\n",
    "    #im = Image.open(filename)\n",
    "    #plt.imshow(im)\n",
    "    \n",
    "    #test_file = \\\n",
    "    #    pathlib.Path(get_project_root() / r\"mlinspect/to_sql/generated_code/pipeline.sql\")\n",
    "    \n",
    "    #with test_file.open(\"r\") as file:\n",
    "    #    test__code = file.read()\n",
    "\n",
    "    #return test__code\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#dbms_connector_p = PostgresqlConnector(dbname=POSTGRES_DB, user=POSTGRES_USER, password=POSTGRES_PW,\n",
    "#                                       port=POSTGRES_PORT, host=POSTGRES_HOST)\n",
    "#print(dbms_connector_p.db_settings)\n",
    "\n",
    "def run_for_all(pipeline_code, mode=\"\", materialize=None):\n",
    "    \n",
    "#    dbms_connector_p = PostgresqlConnector(dbname=POSTGRES_DB, user=POSTGRES_USER, password=POSTGRES_PW,\n",
    "#                                       port=POSTGRES_PORT, host=POSTGRES_HOST)\n",
    "    print(dbms_connector_p.db_settings)\n",
    "\n",
    "    t0 = time.time()\n",
    "    test_code = get_sql_query(pipeline_code=pipeline_code, to_sql=True, dbms_connector=dbms_connector_p, mode=mode, materialize=materialize)\n",
    "    t1 = time.time()\n",
    "    print(\"\\nTime spend with modified SQL inspections (PSQL): \" + str(t1 - t0))\n",
    "    \n",
    "    #t2 = time.time()\n",
    "    #result= get_sql_query(pipeline_code=pipeline_code, to_sql=False)\n",
    "    #t3 = time.time()\n",
    "    #print(\"\\nTime spend with original (pandas): \" + str(t3 - t2))\n",
    "\n",
    "    #(also eventuell könnte ich hier statt get_sql_query oben diesen befehl get_sql_query_for_pipeline von code_as_string.py in example_to_sql benutzen, damit der setup_code und test_code in die files geschrieben wird, aber noch nicht ausgeführt wird!!!)\n",
    "    #(und dann diesen befehl hier laufen lassen, damit die tabellen nicht neu im dbms erzeugt werden) dbms_connector_p.run(test_code)\n",
    "    #print(setup_code + \"\\n\" + test_code)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The code generated using \"CTE\":\n",
    "\n",
    "#run_for_all(pipeline_code, mode=\"CTE\", materialize=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The code generated using \"VIEW\":\n",
    "\n",
    "run_for_all(pipeline_code, mode=\"VIEW\", materialize=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The code generated using \"VIEW\" and \"MATERIALIZED\":\n",
    "\n",
    "#run_for_all(pipeline_code, mode=\"VIEW\", materialize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88cf9573b7b8588226248109e4f46f163d87635f37520c23a7b39ed1f0288615"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
